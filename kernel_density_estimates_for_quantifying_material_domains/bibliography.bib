@article{Palmer2022,
   abstract = {<p>Obtaining accurate estimates of machine learning model uncertainties on newly predicted data is essential for understanding the accuracy of the model and whether its predictions can be trusted. A common approach to such uncertainty quantification is to estimate the variance from an ensemble of models, which are often generated by the generally applicable bootstrap method. In this work, we demonstrate that the direct bootstrap ensemble standard deviation is not an accurate estimate of uncertainty but that it can be simply calibrated to dramatically improve its accuracy. We demonstrate the effectiveness of this calibration method for both synthetic data and numerous physical datasets from the field of Materials Science and Engineering. The approach is motivated by applications in physical and biological science but is quite general and should be applicable for uncertainty quantification in a wide range of machine learning regression models.</p>},
   author = {Glenn Palmer and Siqi Du and Alexander Politowicz and Joshua Paul Emory and Xiyu Yang and Anupraas Gautam and Grishma Gupta and Zhelong Li and Ryan Jacobs and Dane Morgan},
   doi = {10.1038/s41524-022-00794-8},
   issn = {2057-3960},
   issue = {1},
   journal = {npj Computational Materials},
   month = {12},
   pages = {115},
   title = {Calibration after bootstrap for accurate uncertainty quantification in regression models},
   volume = {8},
   url = {https://www.nature.com/articles/s41524-022-00794-8},
   year = {2022},
}
@article{adapt,
   abstract = {ADAPT is an open-source python library providing the implementation of several domain adaptation methods. The library is suited for scikit-learn estimator object (object which implement fit and predict methods) and tensorflow models. Most of the implemented methods are developed in an estimator agnostic fashion, offering various possibilities adapted to multiple usage. The library offers three modules corresponding to the three principal strategies of domain adaptation: (i) feature-based containing methods performing feature transformation; (ii) instance-based with the implementation of reweighting techniques and (iii) parameter-based proposing methods to adapt pre-trained models to novel observations. A full documentation is proposed online https://adapt-python.github.io/adapt/ with gallery of examples. Besides, the library presents an high test coverage.},
   author = {Antoine De Mathelin and François Deheeger and Guillaume Richard and Mathilde Mougeot and Nicolas Vayatis and Centre Borelli},
   keywords = {Deep networks,Domain Adaptation,Fine tuning,Importance weight-ing,Machine learning,Python,Transfer learning},
   title = {ADAPT : Awesome Domain Adaptation Python Toolbox},
   url = {https://adapt-python.github.io/adapt/},
}
@report{Caruana2005,
   abstract = {We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealis-tic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.},
   author = {Rich Caruana},
   title = {Predicting Good Probabilities With Supervised Learning Alexandru Niculescu-Mizil},
   year = {2005},
}
@report{Stahlbock,
   author = {Robert Stahlbock and Gary M Weiss and Mahmoud Abou-Nasr and · Cheng and Ying Yang and Hamid R Arabnia and Leonidas Deligiannidis},
   title = {Transactions on Computational Science and Computational Intelligence Advances in Data Science and Information Engineering},
   url = {http://www.springer.com/series/11769},
}
@article{Scalia2020,
   abstract = {Advances in deep neural network (DNN)-based molecular property prediction have recently led to the development of models of remarkable accuracy and generalization ability, with graph convolutional neural networks (GCNNs) reporting state-of-the-art performance for this task. However, some challenges remain, and one of the most important that needs to be fully addressed concerns uncertainty quantification. DNN performance is affected by the volume and the quality of the training samples. Therefore, establishing when and to what extent a prediction can be considered reliable is just as important as outputting accurate predictions, especially when out-of-domain molecules are targeted. Recently, several methods to account for uncertainty in DNNs have been proposed, most of which are based on approximate Bayesian inference. Among these, only a few scale to the large data sets required in applications. Evaluating and comparing these methods has recently attracted great interest, but results are generally fragmented and absent for molecular property prediction. In this paper, we quantitatively compare scalable techniques for uncertainty estimation in GCNNs. We introduce a set of quantitative criteria to capture different uncertainty aspects and then use these criteria to compare MC-dropout, Deep Ensembles, and bootstrapping, both theoretically in a unified framework that separates aleatoric/epistemic uncertainty and experimentally on public data sets. Our experiments quantify the performance of the different uncertainty estimation methods and their impact on uncertainty-related error reduction. Our findings indicate that Deep Ensembles and bootstrapping consistently outperform MC-dropout, with different context-specific pros and cons. Our analysis leads to a better understanding of the role of aleatoric/epistemic uncertainty, also in relation to the target data set features, and highlights the challenge posed by out-of-domain uncertainty.},
   author = {Gabriele Scalia and Colin A. Grambow and Barbara Pernici and Yi Pei Li and William H. Green},
   doi = {10.1021/acs.jcim.9b00975},
   issn = {1549960X},
   issue = {6},
   journal = {Journal of Chemical Information and Modeling},
   month = {6},
   pages = {2697-2717},
   pmid = {32243154},
   publisher = {American Chemical Society},
   title = {Evaluating Scalable Uncertainty Estimation Methods for Deep Learning-Based Molecular Property Prediction},
   volume = {60},
   year = {2020},
}
@article{Caldeira2020,
   abstract = {We present a comparison of methods for uncertainty quantification (UQ) in deep learning algorithms in the context of a simple physical system. Three of the most common uncertainty quantification methods - Bayesian Neural Networks (BNN), Concrete Dropout (CD), and Deep Ensembles (DE) - are compared to the standard analytic error propagation. We discuss this comparison in terms endemic to both machine learning ("epistemic" and "aleatoric") and the physical sciences ("statistical" and "systematic"). The comparisons are presented in terms of simulated experimental measurements of a single pendulum - a prototypical physical system for studying measurement and analysis techniques. Our results highlight some pitfalls that may occur when using these UQ methods. For example, when the variation of noise in the training set is small, all methods predicted the same relative uncertainty independently of the inputs. This issue is particularly hard to avoid in BNN. On the other hand, when the test set contains samples far from the training distribution, we found that no methods sufficiently increased the uncertainties associated to their predictions. This problem was particularly clear for CD. In light of these results, we make some recommendations for usage and interpretation of UQ methods.},
   author = {João Caldeira and Brian Nord},
   doi = {10.1088/2632-2153/aba6f3},
   issn = {2632-2153},
   issue = {1},
   journal = {Machine Learning: Science and Technology},
   month = {12},
   pages = {015002},
   publisher = {IOP Publishing},
   title = {Deeply uncertain: comparing methods of uncertainty quantification in deep learning algorithms},
   volume = {2},
   year = {2020},
}
@article{Busk2022,
   abstract = {Data-driven methods based on machine learning have the potential to accelerate computational analysis of atomic structures. In this context, reliable uncertainty estimates are important for assessing confidence in predictions and enabling decision making. However, machine learning models can produce badly calibrated uncertainty estimates and it is therefore crucial to detect and handle uncertainty carefully. In this work we extend a message passing neural network designed specifically for predicting properties of molecules and materials with a calibrated probabilistic predictive distribution. The method presented in this paper differs from previous work by considering both aleatoric and epistemic uncertainty in a unified framework, and by recalibrating the predictive distribution on unseen data. Through computer experiments, we show that our approach results in accurate models for predicting molecular formation energies with well calibrated uncertainty in and out of the training data distribution on two public molecular benchmark datasets, QM9 and PC9. The proposed method provides a general framework for training and evaluating neural network ensemble models that are able to produce accurate predictions of properties of molecules with well calibrated uncertainty estimates.},
   author = {Jonas Busk and Peter Bjørn Jørgensen and Arghya Bhowmik and Mikkel N Schmidt and Ole Winther and Tejs Vegge},
   doi = {10.1088/2632-2153/ac3eb3},
   issue = {1},
   journal = {Machine Learning: Science and Technology},
   pages = {015012},
   title = {Calibrated uncertainty for molecular property prediction using ensembles of message passing neural networks},
   volume = {3},
   year = {2022},
}
@article{Abdar2021,
   abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.},
   author = {Moloud Abdar and Farhad Pourpanah and Sadiq Hussain and Dana Rezazadegan and Li Liu and Mohammad Ghavamzadeh and Paul Fieguth and Xiaochun Cao and Abbas Khosravi and U. Rajendra Acharya and Vladimir Makarenkov and Saeid Nahavandi},
   doi = {10.1016/j.inffus.2021.05.008},
   issn = {15662535},
   journal = {Information Fusion},
   keywords = {Artificial intelligence,Bayesian statistics,Deep learning,Ensemble learning,Machine learning,Uncertainty quantification},
   pages = {243-297},
   title = {A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
   volume = {76},
   year = {2021},
}
@article{Koh2020,
   abstract = {Distribution shifts -- where the training distribution differs from the test distribution -- can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. Code and leaderboards are available at https://wilds.stanford.edu.},
   author = {Pang Wei Koh and Shiori Sagawa and Henrik Marklund and Sang Michael Xie and Marvin Zhang and Akshay Balsubramani and Weihua Hu and Michihiro Yasunaga and Richard Lanas Phillips and Irena Gao and Tony Lee and Etienne David and Ian Stavness and Wei Guo and Berton A. Earnshaw and Imran S. Haque and Sara Beery and Jure Leskovec and Anshul Kundaje and Emma Pierson and Sergey Levine and Chelsea Finn and Percy Liang},
   pages = {1-131},
   title = {WILDS: A Benchmark of in-the-Wild Distribution Shifts},
   url = {http://arxiv.org/abs/2012.07421},
   year = {2020},
}
@article{Hirschfeld2020,
   abstract = {Uncertainty quantification (UQ) is an important component of molecular property prediction, particularly for drug discovery applications where model predictions direct experimental design and where unanticipated imprecision wastes valuable time and resources. The need for UQ is especially acute for neural models, which are becoming increasingly standard yet are challenging to interpret. While several approaches to UQ have been proposed in the literature, there is no clear consensus on the comparative performance of these models. In this paper, we study this question in the context of regression tasks. We systematically evaluate several methods on five regression data sets using multiple complementary performance metrics. Our experiments show that none of the methods we tested is unequivocally superior to all others, and none produces a particularly reliable ranking of errors across multiple data sets. While we believe that these results show that existing UQ methods are not sufficient for all common use cases and further research is needed, we conclude with a practical recommendation as to which existing techniques seem to perform well relative to others.},
   author = {Lior Hirschfeld and Kyle Swanson and Kevin Yang and Regina Barzilay and Connor W. Coley},
   doi = {10.1021/acs.jcim.0c00502},
   issn = {15205142},
   issue = {8},
   journal = {Journal of Chemical Information and Modeling},
   pages = {3770-3780},
   pmid = {32702986},
   title = {Uncertainty Quantification Using Neural Networks for Molecular Property Prediction},
   volume = {60},
   year = {2020},
}
@article{Tran2019,
   abstract = {Data science and informatics tools have been proliferating recently within the computational materials science and catalysis fields. This proliferation has spurned the creation of various frameworks for automated materials screening, discovery, and design. Underpinning these frameworks are surrogate models with uncertainty estimates on their predictions. These uncertainty estimates are instrumental for determining which materials to screen next, but the computational catalysis field does not yet have a standard procedure for judging the quality of such uncertainty estimates. Here we present a suite of figures and performance metrics derived from the machine learning community that can be used to judge the quality of such uncertainty estimates. This suite probes the accuracy, calibration, and sharpness of a model quantitatively. We then show a case study where we judge various methods for predicting density-functional-theory-calculated adsorption energies. Of the methods studied here, we find that the best performer is a model where a convolutional neural network is used to supply features to a Gaussian process regressor, which then makes predictions of adsorption energies along with corresponding uncertainty estimates.},
   author = {Kevin Tran and Willie Neiswanger and Junwoong Yoon and Qingyang Zhang and Eric Xing and Zachary W. Ulissi},
   doi = {10.1088/2632-2153/ab7e1a},
   issn = {23318422},
   journal = {arXiv},
   title = {Methods for comparing uncertainty quantifications for material property predictions},
   year = {2019},
}
@article{Morgan2020,
   abstract = {Advances in machine learning have impacted myriad areas of materials science , such as the discovery of novel materials and the improvement of molecular simulations, with likely many more important developments to come. Given the rapid changes in this field, it is challenging to understand both the breadth of opportunities and the best practices for their use. In this review , we address aspects of both problems by providing an overview of the areas in which machine learning has recently had significant impact in materials science, and then we provide a more detailed discussion on determining the accuracy and domain of applicability of some common types of machine learning models. Finally, we discuss some opportunities and challenges for the materials community to fully utilize the capabilities of machine learning.},
   author = {Dane Morgan and Ryan Jacobs},
   keywords = {applicability domain,artificial intelligence,assessment,machine learning,materials design,materials discovery,materials informatics,materials science,model,model errors},
   pages = {1-33},
   title = {Opportunities and Challenges for Machine Learning in Materials Science Keywords},
   url = {https://doi.org/10.1146/annurev-matsci-070218-010015},
   year = {2020},
}

@article{Pernot2022,
   abstract = {Validation of prediction uncertainty (PU) is becoming an essential task for modern computational chemistry. Designed to quantify the reliability of predictions in meteorology, the calibration-sharpness (CS) framework is now widely used to optimize and validate uncertainty-aware machine learning (ML) methods. However, its application is not limited to ML and it can serve as a principled framework for any PU validation. The present article is intended as a step-by-step introduction to the concepts and techniques of PU validation in the CS framework, adapted to the specifics of computational chemistry. The presented methods range from elementary graphical checks to more sophisticated ones based on local calibration statistics. The concept of tightness, is introduced. The methods are illustrated on synthetic datasets and applied to uncertainty quantification data issued from the computational chemistry literature.},
   author = {Pascal Pernot},
   doi = {10.1063/5.0109572},
   issn = {10897690},
   issue = {14},
   journal = {Journal of Chemical Physics},
   month = {10},
   pmid = {36243522},
   publisher = {American Institute of Physics Inc.},
   title = {Prediction uncertainty validation for computational chemists},
   volume = {157},
   year = {2022},
}

@article{Janet2019,
   abstract = {Machine learning (ML) models, such as artificial neural networks, have emerged as a complement to high-throughput screening, enabling characterization of new compounds in seconds instead of hours. The promise of ML models to enable large-scale chemical space exploration can only be realized if it is straightforward to identify when molecules and materials are outside the model's domain of applicability. Established uncertainty metrics for neural network models are either costly to obtain (e.g., ensemble models) or rely on feature engineering (e.g., feature space distances), and each has limitations in estimating prediction errors for chemical space exploration. We introduce the distance to available data in the latent space of a neural network ML model as a low-cost, quantitative uncertainty metric that works for both inorganic and organic chemistry. The calibrated performance of this approach exceeds widely used uncertainty metrics and is readily applied to models of increasing complexity at no additional cost. Tightening latent distance cutoffs systematically drives down predicted model errors below training errors, thus enabling predictive error control in chemical discovery or identification of useful data points for active learning.},
   author = {Jon Paul Janet and Chenru Duan and Tzuhsiung Yang and Aditya Nandy and Heather J. Kulik},
   doi = {10.1039/c9sc02298h},
   issn = {20416539},
   issue = {34},
   journal = {Chemical Science},
   pages = {7913-7922},
   publisher = {Royal Society of Chemistry},
   title = {A quantitative uncertainty metric controls error in neural network-driven chemical discovery},
   volume = {10},
   year = {2019},
}

@article{Jaworska2005,
   abstract = {As the use of Quantitative Structure Activity Relationship (QSAR) models for chemical management increases, the reliability of the predictions from such models is a matter of growing concern. The OECD QSAR Validation Principles recommend that a model should be used within its applicability domain (AD). The Setubal Workshop report provided conceptual guidance on defining a (Q)SAR AD, but it is difficult to use directly. The practical application of the AD concept requires an operational definition that permits the design of an automatic (computerised), quantitative procedure to determine a model's AD. An attempt is made to address this need, and methods and criteria for estimating AD through training set interpolation in descriptor space are reviewed. It is proposed that response space should be included in the training set representation. Thus, training set chemicals are points in n-dimensional descriptor space and m-dimensional model response space. Four major approaches for estimating interpolation regions in a multivariate space are reviewed and compared: range, distance, geometrical, and probability density distribution.},
   author = {Joanna Jaworska and Nina Nikolova-Jeliazkova and Tom Aldenberg},
   doi = {10.1177/026119290503300508},
   issn = {02611929},
   issue = {5},
   journal = {ATLA Alternatives to Laboratory Animals},
   keywords = {Applicability domain,Multivariate interpolation,QSAR},
   pages = {445-459},
   pmid = {16268757},
   title = {QSAR applicability domain estimation by projection of the training set in descriptor space: A review},
   volume = {33},
   year = {2005},
}

@article{Jacobs2020,
   abstract = {As data science and machine learning methods are taking on an increasingly important role in the materials research community, there is a need for the development of machine learning software tools that are easy to use (even for nonexperts with no programming ability), provide flexible access to the most important algorithms, and codify best practices of machine learning model development and evaluation. Here, we introduce the Materials Simulation Toolkit for Machine Learning (MAST-ML), an open source Python-based software package designed to broaden and accelerate the use of machine learning in materials science research. MAST-ML provides predefined routines for many input setup, model fitting, and post-analysis tasks, as well as a simple structure for executing a multi-step machine learning model workflow. In this paper, we describe how MAST-ML is used to streamline and accelerate the execution of machine learning problems. We walk through how to acquire and run MAST-ML, demonstrate how to execute different components of a supervised machine learning workflow via a customized input file, and showcase a number of features and analyses conducted automatically during a MAST-ML run. Further, we demonstrate the utility of MAST-ML by showcasing examples of recent materials informatics studies which used MAST-ML to formulate and evaluate various machine learning models for an array of materials applications. Finally, we lay out a vision of how MAST-ML, together with complementary software packages and emerging cyberinfrastructure, can advance the rapidly growing field of materials informatics, with a focus on producing machine learning models easily, reproducibly, and in a manner that facilitates model evolution and improvement in the future.},
   author = {Ryan Jacobs and Tam Mayeshiba and Ben Afflerbach and Luke Miles and Max Williams and Matthew Turner and Raphael Finkel and Dane Morgan},
   doi = {10.1016/j.commatsci.2020.109544},
   issn = {09270256},
   issue = {October 2019},
   journal = {Computational Materials Science},
   keywords = {Data science,Machine learning,Materials informatics,Open source software},
   title = {The Materials Simulation Toolkit for Machine learning (MAST-ML): An automated open source toolkit to accelerate data-driven materials research},
   volume = {176},
   year = {2020},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{Stanev2018,
   abstract = {Superconductivity has been the focus of enormous research effort since its discovery more than a century ago. Yet, some features of this unique phenomenon remain poorly understood; prime among these is the connection between superconductivity and chemical/structural properties of materials. To bridge the gap, several machine learning schemes are developed herein to model the critical temperatures (T c) of the 12,000+ known superconductors available via the SuperCon database. Materials are first divided into two classes based on their T c values, above and below 10 K, and a classification model predicting this label is trained. The model uses coarse-grained features based only on the chemical compositions. It shows strong predictive power, with out-of-sample accuracy of about 92\%. Separate regression models are developed to predict the values of T c for cuprate, iron-based, and low-T c compounds. These models also demonstrate good performance, with learned predictors offering potential insights into the mechanisms behind superconductivity in different families of materials. To improve the accuracy and interpretability of these models, new features are incorporated using materials data from the AFLOW Online Repositories. Finally, the classification and regression models are combined into a single-integrated pipeline and employed to search the entire Inorganic Crystallographic Structure Database (ICSD) for potential new superconductors. We identify >30 non-cuprate and non-iron-based oxides as candidate materials.},
   author = {Valentin Stanev and Corey Oses and A. Gilad Kusne and Efrain Rodriguez and Johnpierre Paglione and Stefano Curtarolo and Ichiro Takeuchi},
   doi = {10.1038/s41524-018-0085-8},
   issn = {20573960},
   issue = {1},
   journal = {npj Computational Materials},
   month = {12},
   publisher = {Nature Publishing Group},
   title = {Machine learning modeling of superconducting critical temperature},
   volume = {4},
   year = {2018},
}

@article{Wu2017,
   abstract = {We evaluate the performance of four machine learning methods for modeling and predicting FCC solute diffusion barriers. More than 200 FCC solute diffusion barriers from previous density functional theory (DFT) calculations served as our dataset to train four machine learning methods: linear regression (LR), decision tree (DT), Gaussian kernel ridge regression (GKRR), and artificial neural network (ANN). We separately optimize key physical descriptors favored by each method to model diffusion barriers. We also assess the ability of each method to extrapolate when faced with new hosts with limited known data. GKRR and ANN were found to perform the best, showing 0.15 eV cross-validation errors and predicting impurity diffusion in new hosts to within 0.2 eV when given only 5 data points from the host. We demonstrate the success of a combined DFT + data mining approach towards solving materials science challenges and predict the diffusion barrier of all available impurities across all FCC hosts.},
   author = {Henry Wu and Aren Lorenson and Ben Anderson and Liam Witteman and Haotian Wu and Bryce Meredig and Dane Morgan},
   doi = {10.1016/j.commatsci.2017.03.052},
   issn = {09270256},
   journal = {Computational Materials Science},
   keywords = {DFT,Data-mining,Diffusion,Machine learning,Neural network},
   month = {6},
   pages = {160-165},
   publisher = {Elsevier B.V.},
   title = {Robust FCC solute diffusion predictions from ab-initio machine learning methods},
   volume = {134},
   year = {2017},
}
@article{Lu2019,
   abstract = {Machine learning models have been widely utilized in materials science to discover trends in existing data and then make predictions to generate large databases, providing powerful tools for accelerating materials discovery and design. However, there is a significant need to refine approaches both for developing the best models and assessing the uncertainty in their predictions. In this work, we evaluate the performance of Gaussian kernel ridge regression (GKRR) and Gaussian process regression (GPR) for modeling ab-initio predicted impurity diffusion activation energies, using a database with 15 pure metal hosts and 408 host-impurity pairs. We demonstrate the advantages of basing the feature selection on minimizing the Leave-Group-Out (LOG) cross-validation (CV) root mean squared error (RMSE) instead of the more commonly used random K-fold CV RMSE. For the best descriptor and hyperparameter sets, the LOG RMSE from the GKRR (GPR) model is only 0.148 eV (0.155 eV) and the corresponding 5-fold RMSE is 0.116 eV (0.129 eV), demonstrating the model can effectively predict diffusion activation energies. We also show that the ab-initio impurity migration barrier can be employed as a feature to increase the accuracy of the model significantly while still yielding a significant speedup in the ability to predict the activation energy of new systems. Finally, we define r as the magnitude of the ratio of the actual error (residual) in a left-out data point during CV to the predicted standard deviation for that same data point in the GPR model, and compare the distribution of r to a normal distribution. Deviations of r from a normal distribution can be used to quantify the accuracy of the machine learning error estimates, and our results generally show that the approach yields accurate, normally-distributed error estimates for this diffusion data set.},
   author = {Hai Jin Lu and Nan Zou and Ryan Jacobs and Ben Afflerbach and Xiao Gang Lu and Dane Morgan},
   doi = {10.1016/j.commatsci.2019.06.010},
   issn = {09270256},
   journal = {Computational Materials Science},
   keywords = {Diffusion,Error assessment,Gaussian process,Machine learning},
   month = {11},
   publisher = {Elsevier B.V.},
   title = {Error assessment and optimal cross-validation approaches in machine learning applied to impurity diffusion},
   volume = {169},
   year = {2019},
}

 @misc{
    nickel_institute,
    title={Properties of Some Metals and Alloys},
    url={https://nickelinstitute.org/media/1771/propertiesofsomemetalsandalloys_297_.pdf},
    organization={Nickel Institute}
}

@report{domain_adaptation,
   author = {Robert Stahlbock and Gary M Weiss and Mahmoud Abou-Nasr and · Cheng and Ying Yang and Hamid R Arabnia and Leonidas Deligiannidis},
   title = {Transactions on Computational Science and Computational Intelligence Advances in Data Science and Information Engineering},
   url = {http://www.springer.com/series/11769},
}

@report{loco,
   abstract = {The discovery of novel materials drives industrial innovation (1), although the pace of discovery tends to be slow due to the infrequency of "Eureka!" moments (2). These moments are typically tangential to the original target of the experimental work: "accidental discover-ies". Here we demonstrate the acceleration of intentional materials discovery-targeting material properties of interest while generalizing the search to a large materials space with machine learning (ML) methods. We demonstrate a closed-loop ML discovery process targeting novel superconducting materials, which have industrial applications ranging from quantum computing to sensors to power delivery (3-6). By closing the loop, i.e. by experimentally testing the results of the ML-generated superconductivity predictions and feeding data back into the ML model to refine, we demonstrate that success rates for superconductor discovery can be more than doubled (7). In four closed-loop cycles, we discovered a new superconductor in the Zr-In-Ni system, rediscovered five superconductors unknown in the training datasets, and identified two additional phase diagrams of interest for new superconducting materials. Our work demonstrates the critical role experimental feedback provides in ML-driven discovery , and provides definite evidence that such technologies can accelerate discovery even in the absence of knowledge of the underlying physics. Closed-loop machine learning | Superconductivity | Materials discovery},
   author = {Elizabeth A Pogue and Alexander New and Kyle Mcelroy and Nam Q Le and Michael J Pekala and Ian Mccue and Eddie Gienger and Janna Domenico and Elizabeth Hedrick and Tyrel M Mcqueen and Brandon Wilfong and Christine D Piatko and Christopher R Ratto and Andrew Lennon and Christine Chung and Timothy Montalbano and Gregory Bassen and Christopher D Stiles},
   title = {Closed-loop machine learning for discovery of novel superconductors},
}

@article{Meredig2018,
   abstract = {Traditional machine learning (ML) metrics overestimate model performance for materials discovery. We introduce (1) leave-one-cluster-out cross-validation (LOCO CV) and (2) a simple nearest-neighbor benchmark to show that model performance in discovery applications strongly depends on the problem, data sampling, and extrapolation. Our results suggest that ML-guided iterative experimentation may outperform standard high-throughput screening for discovering breakthrough materials like high-Tc superconductors with ML.},
   author = {Bryce Meredig and Erin Antono and Carena Church and Maxwell Hutchinson and Julia Ling and Sean Paradiso and Ben Blaiszik and Ian Foster and Brenna Gibbons and Jason Hattrick-Simpers and Apurva Mehta and Logan Ward},
   doi = {10.1039/c8me00012c},
   issn = {20589689},
   issue = {5},
   journal = {Molecular Systems Design and Engineering},
   month = {10},
   pages = {819-825},
   publisher = {Royal Society of Chemistry},
   title = {Can machine learning identify the next high-temperature superconductor? Examining extrapolation performance for materials discovery},
   volume = {3},
   year = {2018},
}

@software{gpt-3.5,
  author = {OpenAI},
  title = {{GPT-3.5}},
  year = {2021},
  url = {https://openai.com},
}

@article{Kull2017,
   abstract = {For optimal decision making under variable class distributions and misclassification costs a classifier needs to produce well-calibrated estimates of the posterior probability. Isotonic calibration is a powerful non-parametric method that is however prone to overfitting on smaller datasets; hence a parametric method based on the logistic sigmoidal curve is commonly used. While logistic calibration is designed for normally distributed per-class scores, we demonstrate experimentally that many classifiers including Naive Bayes and Adaboost suffer from a particular distortion where these score distributions are heavily skewed. In such cases logistic calibration can easily yield probability estimates that are worse than the original scores. Moreover, the logistic curve family does not include the identity function, and hence logistic calibration can easily uncalibrate a perfectly calibrated classifier. In this paper we solve all these problems with a richer class of parametric calibration maps based on the beta distribution. We derive the method from first principles and show that fitting it is as easy as fitting a logistic curve. Extensive experiments show that beta calibration is superior to logistic calibration for a wide range of classifiers: Naive Bayes, Adaboost, random forest, logistic regression, support vector machine and multi-layer perceptron. If the original classifier is already calibrated, then beta calibration learns a function close to the identity. On this we build a statistical test to recognise if the model deviates from being well-calibrated.},
   author = {Meelis Kull and Telmo M. Silva Filho and Peter Flach},
   doi = {10.1214/17-EJS1338SI},
   issn = {19357524},
   issue = {2},
   journal = {Electronic Journal of Statistics},
   keywords = {Beta distribution,Binary classification,Classifier calibration,Logistic function,Posterior probabilities,Sigmoid},
   pages = {5052-5080},
   publisher = {Institute of Mathematical Statistics},
   title = {Beyond Sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration},
   volume = {11},
   year = {2017},
}

@article{de2021adapt,
  title={ADAPT: Awesome Domain Adaptation Python Toolbox},
  author={de Mathelin, Antoine and Deheeger, Fran{\c{c}}ois and Richard, Guillaume and Mougeot, Mathilde and Vayatis, Nicolas},
  journal={arXiv preprint arXiv:2107.03049},
  year={2021}
}
